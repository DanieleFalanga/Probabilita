\documentclass[12pt]{article}
\usepackage[top=2cm]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{graphicx}


\newtheorem{definition}{Definizione}[section]
\newtheorem{proposition}{Proposizione}[section]
\newtheorem{theorem}{Teorema}[section]
\newtheorem{nota}{Nota}[section]
\newtheorem{notaAMargine}{Nota a margine}[section]

\title{\textbf{Variabili aleatorie}}
\author{Daniele Falanga}
\date{}

\begin{document}
\maketitle

\section{Variabili aleatorie}

Una variabile aleatoria è una funzione che associa ad ogni esperimento
un valore numerico. Possono essere di due tipi: 
\begin{itemize}
    \item Continue: che associa una serie infinita di valori all'interno 
    di un intervallo
    \item Discrete: che associa un numero finito e numerabile di valori
\end{itemize}

\begin{definition}
    La funzione di ripartizione F di una variabile aleatoria X, 
    è definita, per ogni numero
    reale x, tramite: 
    \[
    F(x) = P(X \le x)    
    \]
    La funzione \(F(x)\) esprimre la probabilità che la variabile aleatoria X assuma un valore minore o uguale ad \(x\)
\end{definition}

La funzione di ripartizione è una descrizione completa della distribuzione di probabilità di una variabile casuale.
In sostanza, la funzione di ripartizione di una variabile aleatoria ci dice come è 
ripartita in un tinervallo la probabilità della variabile. 

\section{Variabili aleatorie discrete e continue}

\begin{definition}
    Se X è una variabile aleatoria discreta, la sua funzione di massa di probabilità si definisce in questo modo
    \[
    p(a) = P(X = a)    
    \]
\end{definition}

Fornisce il valore della probabilità quando la variabile \(X\) assume un determinato valore
nell'intervallo. Infatti i suoi valori possibili sono:
\begin{align*}
    p(x_i) > 0, & \quad i = 1,2,\dots \\
    p(x) = 0, & \quad \text{tutti gli altri valori di x}
\end{align*}

Infatti deve valere necessariamente questa relazione: 
\[
\sum_{i=1}^{\infty} = 1    
\]
\newpage

Per una variabile aleatoria discreta, la funzione di ripartizione può essere riscritta
in funzione della funzione di massa: 
\[
F(a) = \sum_{x \le a} = p(x)    
\]

Caso differente per le variabili aleatorie {\bf{continue}}. 
\begin{definition}
    Si definisce densità della variabile aleatoria continua X
    \[
    P(X \in B) = \int_{B} f(x)dx    
    \]
\end{definition}

La sua densità deve rispettare: 
\[
    1 = P(X \in \mathbb{R} ) = \int_{-\infty}^{\infty} f(x)dx
\]

La relazione che lega la funzione di ripartizione F alla densita \(f\) è la seguente: 
\[
F(a) = P(X \in (-\infty,a]) = \int_{-\infty}^af(x)dx    
\]

se si derivano entrambi i membri si ottiene che la densità è la derivata
della funzione di ripartizione: 
\[
\frac{d}{da}F(a) = f(a)    
\]

\section{Coppie e vettori di variabili aleatorie}
Non sempre una sola variabile aleatoria va bene, quindi si tende ad 
usarne 2 o più.

\begin{definition}
    Siano X e Y due variabili aleatorie di uno stesso esperimento.
    Si dice funzione di ripartizione congiunta delle variabili X ed Y la seguente funzione:
    \[
    F(x,y) := P(X \le x, Y \le y)   
    \]
\end{definition}

\subsection{Distribuzione congiunta per variabili aleatorie discrete}

La funzione di massa di due varibili aleatorie si definisce: 
\begin{definition}
    Se X e Y sono variabili aleatorie discrete che assumono valori 
    \(x_1,x2\dots\) e \(y_1,y_2,\dots\) rispettivamente, la funzione:
    \[
    p(x_i,y_j) := P(X=x_i,Y=y_j), \quad i = 1,2,\dots, \quad j=1,2,\dots    
    \]    
    è la funzione di massa di probabilità congiunta
    
\end{definition}

Le funzioni di massa di probabilità individuali, si possono ricavare da 
quella congiunta, se per esempio volessi calcolare la funzione di massa di X
al evento \(x_i\) mi basta fissare x e unire gli eventi y.
\[
P_X =(x_i) = \sum_jp(x_i,y_j)    
\]

In modo del tutto analogo per y. 
\newline
{\bf{Non vale il viceversa !!!}} 
Non posso calcolare la funzione di massa congiunta da quelle individuali.

\subsection{Variabili aleatorie indipendendenti}

Due variabili sono indipendendenti se ogni evento relativo alla prima variabile è indipendendente
dalla seconda

\begin{definition}
    Due variabili aleatorie che riguardano lo stesso esperimento casuale si dicono indipendendenti
    se, per ogni coppia di insiemi di numeri reali A e B è soddisfatta l'equazione: 
    \[
    P(X \in A, Y \in B) = P(X \in A)P(Y \in B)    
    \]
\end{definition}

La funzione di ripartizione congiunta è il prodotto delle marginali:
\[
F(a,b) = F_X(a)F_Y(b)
\]

Anche la funzione di massa congiunta è il prodotto delle marginali: 
\[
p(x,y) = p_X(x)p_Y(y)    
\]

\subsection{Distribuzioni condizionali}

\begin{definition}
    Siano X e Y due varibili aleatorie discrete con funzione di massa congiunta \(p(.,.)\).
    Si dice funzione di massa di probabilità condizionata di X dato Y, e si indica con \(p_{X|Y}(.|.)\),
    la funzione a due variabili cosi definita:
    \begin{align*}
        p_{X|Y}(x|y) := P(X=x|Y=y) \\
        = \frac{P(X=x,Y=y)}{P(Y=y)}   \\
        = \frac{p(x,y)}{p_Y(y)}    
    \end{align*}
\end{definition}

\section{Valore Atteso}
È uno dei concetti fondamentali della teoria della probabilità. 
\begin{definition}
    Sia X una variabile aleatoria discreta che puo assumere i valori \(x_1,x_2,\dots\); il valore 
    atteso di X, se esiste (deve convergere la sommatoria) è definito come segue:
    \[
    E[X] := \sum_{i}x_iP(X = x_i)    
    \]

    In pratica, la media pesata che i dei possibili valori di X, dove il peso è la probabilità
    che X assuma proprio quel valore. 
\end{definition}

\begin{nota}
Il concetto di valore atteso è analogo a quello del baricentro in fisica
\end{nota}

\subsection{Proprietà del valore atteso}
Data una variabile aleatoria X e una sua distribuzione, in molti casi è utile voler calcolare
il valore atteso di una sua funzione \(g(X)\), anch'essa una variabile aleatoria.

\begin{proposition}
    Se X è una variabile aleatoria discreta con funzione di massa di probabilità \(p\), allora,
    per ogni funzione \(g\):
    \[
    E[g(x)] = \sum_{x}g(x)p(x)    
    \]    
\end{proposition}

\subsubsection{Valore atteso di una somma di variabili aleatorie}

La versione in due dimensioni della funzione di variabile aleatoria:
\[
E[g(X,Y)] = \sum_x \sum_y g(x,y) p(x,y)    
\]

Si può applicare anche a \(g(X,Y) =  X+Y\):
\[
E[X+Y] = E[X]+E[Y]    
\]

\newpage
\section{Varianza}

Data una variabile aleatoria X, è utile, oltre calcolare il la sua media (\(E[X]\)), vedere quanto
i dati si discostano dalla media, ed è possibile farlo tramite la varianza. 

\begin{definition}
    Sia X una variabile aleatoria con media \(\mu\). la varianza di X, si denota con Var(X), 
    è la quantità: 
    \[
    Var(X) := E[(X-\mu)^2]    
    \]

    Come formula alternativa, ed anche \underline{più utile} nel nostro caso:
    \[
        Var(X) := E[X^2]- E[X]^2
    \]
\end{definition}

Una utile identità per la covarianza:
\[
Var(aX+b) = a^2Var(X)    
\]

Si definisce \textbf{deviazione standard} della variabile X: 
\[
\sqrt{Var(X)}    
\]

La varianza, in termini fisici, corrisponde al momento di inerzia. 

\section{La covarianza e la varianza della somma di variabili aleatorie}

Da vedere se le formule a pagina 135 servono 

\textbf{Attenzione !!!} 
\newline
Per la varianza, la media delle somme, non coincide con la somma delle medie. 

La covarianza indica la relazione lineare tra due varibili aleatorie, in altre parole, indica
se le variazioni di una variabile sono associate a quelle dell' altra variabile. 
\newline
Se la covarianza è positiva vuol dire che sono direttamente proporzionali, altrimenti sono inversamente
proporzionali
\begin{definition}[Covarianza]
    Siano assegnate due variabili aleatorie X ed Y di media \(\mu_x\) e \(\mu_y\) rispettivamente. 
    La loro \underline{covarianza}, che si indica con Cov(X,Y) è la quantità:
    \[
    Cov(X,Y) := E[(X-\mu_X)(Y-\mu_Y)]    
    \]

    Come formula alternativa:
    \[
        Cov(X,Y) := E[XY] - E[X]E[Y]
    \]
\end{definition}

\begin{theorem}
    Se X e Y sono variabili aleatorie indipendendenti, allora:
    \[
    E[XY] = E[X]E[Y]    
    \]
\end{theorem}

La loro covarianza, essendo indipendenti sarà uguale a 0 

\newpage

Come detto prima, la covarianza di due variabili aleatorie, se è positiva, vuol dire
che le variabili sono direttamente proporzionali, viceversa, inversamente proporzionali. 

La forza di correlazione tra le due variabili, è misurata più propriamente dal
\textbf{coefficente di correlazione} definito come segue: 
\[
Corr(X,Y) := \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}    
\]

L'indice di correlazione è \textbf{sempre} compreso tra -1 e 1.

\section{Legge debole dei grandi numeri}

Prima di esporre la legge debole dei grandi numeri vanno preliminarmente esposte
le due proposizioni seguenti: 

\begin{proposition} [Disuguaglianza di Markov]
    Se X è una variabile aleatoria non negativa, allora per ogni \(a > 0\):
    \[
    P(X \ge a) \le \frac{E[X]}{a}    
    \]
\end{proposition}
La dimostrazione non credo si debba fare, ma la metto comunque
\begin{proof}[Dimostrazione]
    \begin{align*}
        E[X] := & \int_{0}^{\infty} xf(x)dx \\
        = & \int_{0}^{a}f(x)dx + \int_{a}^{\infty}f(x)dx \\
        \ge & \int_{a}^{\infty}xf(x)dx \quad \text{Perchè il primo addendo è positivo}\\
        \ge & \int_{a}^{\infty}af(x)dx \quad \text{Perchè x è maggiore di a nella regione di integrazione} \\
        = &  a\int_{a}^{\infty}f(x)dx \\
        = &  aP(X \ge a)
    \end{align*}
\end{proof}

In altre parole, la probabilità che una variabile casuale non negativa superi un certo valore a è limitata superiormente dal rapporto tra il valore atteso della variabile casuale e il valore a
\newpage
\begin{proposition}[Disuguaglianza di Chebyshev]
    Se X è una variabile aleatoria con media \(\mu\) e varianza \(\sigma^2\),
    allora per ogni \(r > 0\),: 
    \[
    P([X-\mu] \ge r) \le \frac{\sigma^2}{r^2}    
    \]
    oppure in modo analogo: 
    \[
    P([X-\mu] > k\sigma) \le \frac{1}{k^2}    
    \]
\end{proposition}

\begin{proof}[Dimostrazione]
    Gli eventi \(\{|X-\mu|\ge r\}\) e \(\{(X-\mu)^2 \ge r^2\}\) coincidino e si dicono 
    equiprobabili. 
    \newline
    Visto che \((X-\mu)^2\) è una variabile aleatoria non negativa, possono applicare Markov:
    con \(a = r^2\)
    \[
    P(|X-\mu| \ge r) = P((X-\mu)^2 \ge r^2) \le \frac{E[(X-\mu)^2]}{r^2} = \frac{\sigma^2}{r^2}    
    \] 
\end{proof}

La disuguaglianza di Chebyshev fornisce una misura di quanto una 
variabile casuale si discosta dalla sua media. 
Indica che, a prescindere dalla forma precisa della distribuzione, 
la maggior parte dei valori della variabile casuale si trova entro un certo 
numero di deviazioni standard dalla media. Più grande è il valore di k, 
più stretto diventa l'intervallo in cui la maggior parte dei valori si concentra.

\subparagraph*{Legge debole dei grandi numeri}

\begin{theorem}
    Sia \(X_1,X_2,\dots\) una successione di variabili aleatorie indipendenti e 
    identicamente distribuite, tutte con media \(E[X_i] = \mu\). Allora per ogni \(\epsilon\):
    \[
    P \left( \left| \frac{X_1+\dots+X_n}{n} + \mu \right| > \epsilon \right) \longrightarrow 0 \quad n \longrightarrow \infty    
    \]
\end{theorem}
\begin{proof}[Dimostrazione]
    Supponendo che le X abbiano varianza \(\sigma^2\) finita:
    \begin{align*}
        E \left[ \frac{X_1+\dots+X_n}{n} \right] = \mu &  &Var\left( \frac{X_1+\dots+X_n}{n} \right) = \frac{\sigma^2}{n} 
    \end{align*}
    La seconda si trova in questo modo: 
    \begin{align*}
        Var\left( \frac{X_1+\dots+X_n}{n} \right) = & \frac{1}{n^2} Var(X_1+\dots+X_n) \\
        = & \frac{Var(X_1)+\dots+Var(X_n)}{n^2} \\ 
        = & \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}
    \end{align*}

    Segue, applicando a Chebyshev la variabile aleatoria \((X_1,\dots,X_n)/n\) che:
    \[
    P \left( \left|\frac{X_1+\dots+X_n}{n}-\mu \right|> \epsilon\right) \le \frac{\sigma^2}{n\epsilon^2}    
    \]
\end{proof}

\end{document}